title: A Modest Proposal To Save the World
---
pub_date: 2018-12-07
---
author: James Webber
---
published: true
---
body:

__Warning__: what follows are the hare-brained ideas of a mad<sidenote>Or at least fairly irate.</sidenote> scientist. Before undertaking any dangerous, untested geo-engineering project that has the potential to destroy life as we know it, you should consult with your physician, physicist, psychic, and/or psychiatrist.

I want to specifically emphasize that there are a lot of open questions and missing details in the ideas below. This essay is not meant as a solution<sidenote>And definitely not as a magic bullet that removes the need for other solutions! If stupid conservatives glom onto this as a way to avoid responsibility I will be quite put out.</sidenote> but rather as an exhortation to undertake the research that can answer these questions while there is still time for the answers to be useful.

- - -

## Climate change is the biggest catastrophe in history

There are so many terrible things happening all at once, it's hard to keep track of all them. Originally I had a long section here laying out how bad climate change is, but you probably know all of that already.<marginnote>In case you managed to forget for a few seconds: the world is warming at an alarming rate. We just got the [latest report](https://nca2018.globalchange.gov/) on the danger, but there have been reports like that for a long time. It's hard to imagine the emissions picture changing in the near term, and we only have [about a decade](http://www.ipcc.ch/report/sr15/) if we want to avoid the worst consequences. We need a Green New Deal pronto, but even if we can start that tomorrow we should be thinking of what else we can do to reduce the damage.</marginnote> I'll keep it short and assume you're onboard with the title of this section.

The looming disaster scenarios and our collective failure to take action has led a lot of people to think about options that don't involve reaching a global agreement on energy conservation. Most of these options involve ways of getting CO<sub>2</sub> out of the atmosphere and sticking it somewhere safe. Unfortunately most of these options are pretty lousy. The big problem is scale. Since the industrial revolution we've added about 1600 gigatons of carbon dioxide into the atmosphere<sidenote>source: [this infographic and their sources](https://informationisbeautiful.net/visualizations/how-many-gigatons-of-co2/)</sidenote>. We continue to dump more at a rate of >30Gt CO<sub>2</sub> per year<sidenote>source: [IEA](https://www.iea.org/geco/emissions/)</sidenote>.

If sequestration is going to work, it has to work at the multi-gigaton scale. I haven't heard of any sequestration plan that can come close to capturing that amount of carbon: the typical proof of concept is minuscule, and realistic industrial application would only be a drop in the bucket compared to global emissions. In general, any energy-intensive method of sequestration runs into a green twist on the [rocket equation](https://en.wikipedia.org/wiki/Tsiolkovsky_rocket_equation): you'll do a lot of work to offset all the carbon you emitted to do the sequestration itself, and if you aren't extremely efficient this will massively increase the amount of effort needed in the end.<marginnote>Another big strategy is to [block some of the sunlight hitting us](http://iopscience.iop.org/article/10.1088/1748-9326/aae98d). I consider that plan slightly more crazy and outlandish than the one I'm going to outline below. And for what it's worth, it would likely be far more expensive.</marginnote>

Furthermore, most technologies are not scalable as long as humans are needed: if sequestering one ton of carbon requires one person to work for one hour, we'll need >15 million people working full-time to offset our current emissions, with no source of funding in sight. A practical sequestration strategy needs to be "too cheap to meter"&mdash;so efficient that the cost per ton goes to zero.

- - -

## Cyanobacteria: processing carbon at scale

Perhaps predictably, this is where a biologist says that biology offers a solution. While biological organisms are frustratingly messy and chaotic and difficult to understand, they're also amazingly efficient, at a scale that no human technology can touch. I'm certainly biased, but I can't think of a more plausible way to sequester carbon than to engineer [cyanobacteria](https://en.wikipedia.org/wiki/Cyanobacteria)<sidenote>Sometimes known as "blue-green algae", but they're bacteria rather than proper algae.</sidenote> to more efficiently capture sunlight and fix carbon into sugar, where it can be introduced to the food chain and sequestered for the long term by larger organisms.

A promising candidate for this type of project is [_Prochlorococcus marinus_](https://en.wikipedia.org/wiki/Prochlorococcus), a.k.a. photosynthetic picoplankton. These are some of most abundant organisms on Earth, with an estimated $2.8 - 3 x 10^{27}$ living in the world's oceans. To put that number in perspective:

    Humans (est.)                                7,000,000,000
    Human cells                  2,100,000,000,000,000,000,000
    Prochlorococcus      3,000,000,000,000,000,000,000,000,000 (!)

At $3 x 10^{-14}$ g of carbon per cell<sidenote>source: [Cermak _et al._](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5322313/)</sidenote>, the existing population of picoplankton contains roughly 81 megatons of carbon. If they divide once per day, that adds up to about 30Gt per year<sidenote>This is a very rough estimate, but it's on the same order as real research on the role of picoplankton in the carbon cycle: see [Johnson & Zinser _et al._](http://science.sciencemag.org/content/311/5768/1737)</sidenote>. To offset our current emissions, we'd need to roughly double that cycling population, assuming the excess population was being absorbed into the oceanic ecosystem and thereby sequestered<sidenote>This is a huge assumption and one of those "open questions" I mentioned at the beginning.</sidenote>.

We know that this could at least theoretically work, because it's the reason that our atmosphere isn't full of carbon dioxide in the first place. Roughly 2.45 billion years ago, some early carbon-fixing organisms grew at such a prodigious rate that they converted our atmosphere from methane- and CO<sub>2</sub>-rich to O<sub>2</sub>-rich. This wiped out a lot of the existing (anaerobic) life on Earth, in what's been called [the Oxygen Catastrophe](https://en.wikipedia.org/wiki/Oxygen_Catastrophe). Such is the power of little microbes in big numbers.
<marginnote>__Update__ (2018-12-12): I made a mistake which I often make, which is that I conflated gigatons of carbon with gigatons of CO<sub>2</sub>. 81 megatons of carbon represents about _300_ megatons of carbon dioxide, because most of the mass in the latter comes from oxygen. This is probably canceled out by an overestimate of the average growth rate, which is how I end up near the literature estimates for total production.</marginnote>

What's even more amazing about this is that they pulled it off with such a lousy enzyme playing the key part.

- - -

## RuBisCO is a bad enzyme and it should feel bad

The most abundant enzyme on Earth is [RuBisCO](https://en.wikipedia.org/wiki/RuBisCO): the protein responsible for converting carbon dioxide into something life can metabolize. The reason it's so abundant is not only because of its important role&mdash;it's because RuBisCO is not very good at capturing carbon dioxide, and it is usually the rate-limiting step in the photosynthetic pathway.<marginnote>RuBisCO's inefficiency is largely because it has a difficult time distinguishing between carbon dioxide and oxygen&mdash;if you can imagine the shape of each molecule, you might see why they'd be hard to tell apart. If the enzyme accidentally grabs an O<sub>2</sub> molecule, it still reacts, but it creates unproductive byproducts that the organism must spend additional energy cleaning up. It's hard for the enzyme to become better at binding CO<sub>2</sub> without also increasing this byproduct effect. Research suggests that most plants have a version of RuBisCO that is closely adapted to the CO<sub>2</sub> and O<sub>2</sub> concentrations of their natural habitat (see [Studer _et al._](http://www.pnas.org/content/111/6/2223)), and efforts to engineer the enyzme toward higher efficiency have been mostly fruitless (pun intended).</marginnote>

It seems strange that such an important enzyme would be so inefficient, but it makes a little more sense in light of the evolutionary history that led to its existence. At the dawn of photosynthetic life, there was essentially no molecular oxygen in the atmosphere and oceans. Thus, there wasn't any pressure to become particularly selective for CO<sub>2</sub> over O<sub>2</sub>, and organisms with RuBisCO were just as good as any alternative. Billions of years later, conditions are quite different, but there's no evolutionary path for an organism to develop a better version of the enzyme. We'll have to engineer one, instead.

In the oceans, dissolved CO<sub>2</sub> exists in an equilibrium with [carbonic acid](https://en.wikipedia.org/wiki/Carbonic_acid) (H<sub>2</sub>CO<sub>3</sub>, or H<sup>+</sup> & HCO<sub>3</sub><sup>-</sup> [a.k.a. bicarbonate]). This is the acid in the phrase "[ocean acidification](Ocean acidification)", another downside to emitting gigatons of CO<sub>2</sub>. An enzyme called [carbonic anhydrase](https://en.wikipedia.org/wiki/Carbonic_anhydrase) catalyzes the interconversion between carbon dioxide and bicarbonate. Animals like us use this enzyme to maintain the pH balance in our blood and tissues, but plants also use it for something else: to increase the local concentration of CO<sub>2</sub> in their chloroplasts, so that all of their RuBisCO enzymes can operate as efficiently as possible. In contrast with RuBisCO, carbonic anhydrases are _incredibly_ good at their jobs: they are typically diffusion-limited, meaning that the enzyme works faster than the molecules involved can get out of the way. I find it a bit amazing to think about these two enzymes, interacting with such closely related molecules at vastly different levels of efficiency.

- - -

## The three-legged stool of bio-geoengineering

Enough background. Here are the three steps to engineering more efficient photosynthesis into picoplankton. Each one of these steps is a major research project that should be spread over many groups, working collaboratively but not necessarily in coordination (as no one can predict what approach will work best). Given the current state of scientific knowledge I believe all three of these steps are _possible_, although it's hard to guess how long they'd take or how much work they might require. I'll introduce them in what I believe to be increasing order of difficulty:

### 1. Create a genetically isolated strain of _Prochlorococcus marinus_

Most of the fears about genetically modified organisms are nonsense&mdash;GMO crops are safe to eat and safe to grow, although they don't make the problems of industrial agriculture go away on their own. In the case of genetically modifying bacteria, and _especially_ in the case where we're trying to engineer a completely new and potentially very powerful metabolic pathway, it's much more reasonable to be concerned. For that reason, any work along these lines should take two different strategies to maintain isolation.

The first step is to synthesize a strain of _Prochlorococcus_ with a new [genetic code](https://en.wikipedia.org/wiki/Genetic_code), one in which the codons for several pairs of amino acids have been swapped with one another<sidenote>e.g. swap all the codons for alanine with those for serine, and modify the corresponding [tRNA](https://en.wikipedia.org/wiki/Transfer_RNA) to match</sidenote>. The result of this recoding would be an organism that is genetically isolated from all other life on Earth: its own genome is indecipherable to any organism using the standard code, and any new DNA it incorporates will be likewise useless for its own translation machinery. Recoding several pairs of amino acids will prevent the engineered strain from ever sharing genes with other organisms in the ocean<sidenote>It's important to do this thoroughly, because the number of these organisms is so huge that every mutation occurs almost immediately. At $3x10^{27}$ existing organisms in the ocean, I estimate that every set of three nucleic acids in the _Prochlorococcus_ genome is mutated in at least one organism, _every minute_. Genetic isolation at that scale means setting up a wall of mutations that must all happen simultaneously for the progeny to remain viable.</sidenote>.

The second step, for use during research and development of the organism, is to use [non-standard amino acids](https://en.wikipedia.org/wiki/Expanded_genetic_code#Non-standard_amino_acids) (NSAAs) to keep this strain confined to the lab until we are confident it should be released (see step 3). Using a NSAA is a good way to keep something from growing outside the lab, but in this case we _need_ our strain to grow outside the lab, so it can't be the long-term solution. At the scale necessary for carbon sequestration, we wouldn't be able to keep feeding it synthetic nutrients. On the other hand, recoding the genome only keeps it genetically isolated, not physically. So both measures are necessary until the design is complete.

Those tasks might seem like complete science fiction, but I list them first because all of the pieces have already been demonstrated by existing researchers. They've [synthesized entire genomes](https://en.wikipedia.org/wiki/Mycoplasma_laboratorium) on a similar scale to that of _Prochlorococcus_. They've [introduced synthetic metabolites](https://www.nature.com/articles/nature14121) to create strains that can't grow without specific additives. Researchers have even demonstrated that [recoded organisms can be genetically isolated from horizontal gene transfer](https://elifesciences.org/articles/34878), although not yet at the scale needed for this project. Creating a fully codon-swapped genome for _Prochlorococcus_ is a tall order, but it's not too far beyond the cutting edge of research being done today. It would take time and money to get it done, but it's well within the realm of possibility<sidenote>Which is pretty amazing, when you realize that the idea is to create organisms with an _entirely new genetic code_.</sidenote>.

### 2. Build a better RuBisCO

The second step is a _lot_ more audacious, but we're closer to having the tools than ever before. The basic goal is to change the beginning of the carbon-fixing pathway from CO<sub>2</sub> to HCO<sub>3</sub><sup>-</sup>, which we know can be bound with high efficiency by carbonic anhydrase. This might sound simple to you<sidenote>Or impossibly difficult, depending on your background.</sidenote>, but engineering new enzymes has proven to be extremely challenging, even as our ability to engineer new proteins and simulate protein structure has gotten much better. That's partially because enzymes are dynamic machines that go through conformational changes as they catalyze a reaction&mdash;we can design a protein that's very stable in one conformation, but getting it to cycle between multiple different states is much trickier.

I'm no expert in this field, but I do know some experts<sidenote>Largely skewed towards UCSF, because that's where I did my graduate work.</sidenote>. In my estimation the front-runners in _de novo_ protein design are those working in the [Rosetta](https://www.rosettacommons.org/) family<sidenote>started by [David Baker](https://www.bakerlab.org/), now at the University of Washington.</sidenote>, using machine learning and clever search algorithms to explore protein space<sidenote>[Tanja Kortemme's group](http://kortemmelab.ucsf.edu/) has done research on stabilizing multiple conformations simultaneously as a way to design new enzymes.</sidenote>. There are many other promising areas of research, though, including new ideas in molecular dynamics<sidenote>e.g. [Michael Grabe's group](http://www.cvri.ucsf.edu/~grabe/research.html) has done research on efficiently sampling the structures of enzymatic processes using molecular dynamics.</sidenote>. Just recently Google announced the results of their [AlphaFold](https://deepmind.com/blog/alphafold/) project, which is using deep learning to predict protein structure from sequence. Their initial results look incredibly promising, although it's not clear if their approach is useful for designing new proteins.

Beyond the computational work, there are also many groups developing high-throughput methods for synthesizing and testing proteins. Almost every computationally-designed protein needs to go through lots of optimization in a real organism before it reaches its full potential, and these methods are going to be at least as important for developing our carbon-fixing machine<sidenote>I know even less about this topic but I'll mention [Polly Fordyce](http://www.fordycelab.com/) and [Jennifer Cochran](http://cochranlab.net/), both at Stanford, as two investigators doing amazing stuff in this area.</sidenote>.

It's hard to guess at how long this step might take, or how much it would take in terms of resources. It might turn out to be unfeasibly complicated, as one new enzyme requires another and another, multiplying the complexity. It might turn out to be entirely impossible, but I don't think that's the case. But we'll never have any idea if we don't put the resources into finding out. The worst-case risk is that we learn something useful about enzyme design and carbon metabolism, which isn't too bad all things considered.

Strictly in terms of money, funding researchers to do this kind of work is fairly cheap&mdash;maybe a few billion dollars over a decade. Compared to the cost of cleaning up climate-related disasters, it's _trivial_<sidenote>The damage from the [most recent California wildfire](https://en.wikipedia.org/wiki/Camp_Fire_(2018)) is estimated at $7.5-10 billion, and that itself is small compared to a bad hurricane or flood.</sidenote>. This is a gamble that may not pay off the way we hope, but we're getting some very good odds.

### 3. Establish ecological efficacy and safety

I list this step last because it's the most important as well as the most difficult. Releasing an engineered organism into the world's largest ecosystem is not a decision to take lightly. We should only do so if we have thoroughly explored the risks involved and weighed them against the potential benefits.

The potential benefits are pretty clear, although we'll have to make a level-headed assessment of how likely they are. Doubling the flux of carbon into the ocean is not going to fix the climate instantly, and we probably have many years of warming to go even in the best case. But we could hope to stabilize ocean acidity, which could help ameliorate the bleaching of coral reefs and the threat to many crustaceans. Depending on the amount of flux, we might hope to not only stabilize our carbon emissions but to pull out some of the excess CO<sub>2</sub> emitted over the past two centuries.

Properly estimating the potential benefit is work for ecologists, climatologists, and geologists&mdash;the same people who are already working to help us prepare for the next century of climate change. There is a lot of research out there estimating how carbon is processed in the ocean, how it filters through the ecosystem, and how it is eventually either sequestered or is re-released into the atmosphere. I won't attempt to summarize said research, but all of it will be important for modeling the effects of such a large intervention in the environment.

The potential risks are more open-ended. There are ecological risks that would need to be assessed&mdash;hopefully the fact that _Prochlorococcus_ is at the bottom of the oceanic food chain would minimize some risk, but it's not clear. It still exists in an ecosystem that may be thrown out of balance by this new arrival (for example, _Prochlorococcus_ coexists with [_Synechococcus_](https://en.wikipedia.org/wiki/Synechococcus) and the relationship between the two is unclear). CO<sub>2</sub> is not usually the limiting nutrient for plankton, so one question is whether there is even capacity in the ocean for such an expansion of the picoplankton population<sidenote>But it's worth noting that a vastly-more-efficient carbon fixation pathway could change the calculation for limiting nutrients.</sidenote>.

Perhaps the most obvious risk is in succeeding _too well_, and pulling more CO<sub>2</sub> out of the atmosphere than we ever put in. Previously I mentioned the Great Oxygenation Event (a.k.a. the Oxygen Catastrophe), when early photosynthesizing organisms removed nearly all the methane and carbon dioxide from the atmosphere. The effect of this was a runaway ice age, the [Huronian glaciation](https://en.wikipedia.org/wiki/Huronian_glaciation), that lasted 300 million years and caused mass extinctions. Obviously we don't want to do that<sidenote>[This documentary](https://www.imdb.com/title/tt1706620/) suggests it wouldn't be pleasant.</sidenote>.

There are a variety of different solutions for this that need to be explored. We could try to engineer a limited number of divisions into our new strain of picoplankton, although with such a huge number of divisions it's very likely to mutate away from any biological switch<marginnote>Jumping to a different domain of life: in organisms like ourselves, the ends of our chromosomes ([telomeres](https://en.wikipedia.org/wiki/Telomere)) get shorter every time our cells divide, and continual growth requires enzymes called [telomerases](https://en.wikipedia.org/wiki/Telomerase) to lengthen them. If we encoded a telomerase to require NSAAs, the organism could be grown in the lab with synthetic nutrients but would have a ticking clock as soon as it was unable to extend its telomeres.</marginnote>. We could strive to engineer our carbon-fixing enzyme very carefully, such that it loses efficiency as the carbon concentration goes back to normal levels. Again, the organism will have strong incentive to mutate away from any obstacles, but it's possible that we could engineer a local minima that was difficult to escape. In either case, we can use a version that relies on non-standard amino acids to test its ability to evade our control mechanisms&mdash;NSAAs aren't economical at global scale but could be used to grow trillions of organisms in a controlled setting so that we can explore its mutational landscape.

- - -

## No easy answers

In the end, there's no way to prove that a geoengineering approach like this is risk-free. On the other hand, the risk of doing nothing is extreme. As a global society we will likely be faced with choosing between a known-dangerous future or a potentially-dangerous solution. I don't know how to answer that question for anyone else, except to advocate that we start exploring the options as soon as possible, before it's too late. And while the outline I just laid out is pretty crazy, it seems a lot saner than hoping the world is going to fix itself anytime soon. In summary:

> ### I know I just met you,
> ### and this is **crazy**,
> ### but we're facing the biggest ecological disaster in recorded history,
> ### so let's try this, maybe?
